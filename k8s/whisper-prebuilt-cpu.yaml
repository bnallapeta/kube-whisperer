apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    sidecar.istio.io/inject: "false"
  name: whisper-service-cpu
  namespace: default
spec:
  predictor:
    containers:
    - env:
      # Model configuration
      - name: WHISPER_MODEL
        value: base  # Options: tiny, base, small, medium, large
      - name: DEVICE
        value: cpu  # CPU-only deployment
      - name: COMPUTE_TYPE
        value: int8  # int8 is faster on CPU
      - name: CPU_THREADS
        value: "8"  # Increase CPU threads for better performance
      - name: NUM_WORKERS
        value: "2"  # Number of worker processes
      
      # Service configuration
      - name: LOG_LEVEL
        value: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
      - name: TEMP_DIR
        value: "/tmp/whisper_audio"
      - name: MODEL_DOWNLOAD_ROOT
        value: "/tmp/whisper_models"
      - name: MAX_FILE_SIZE
        value: "26214400"  # Max file size in bytes (25MB)
      
      image: ghcr.io/bnallapeta/kube-whisperer:0.0.1
      resources:
        limits:
          cpu: "8"
          memory: 8Gi
        requests:
          cpu: "4"
          memory: 4Gi 